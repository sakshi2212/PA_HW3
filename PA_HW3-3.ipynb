{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection and Data Classification Assignment**"
      ],
      "metadata": {
        "id": "-YWrMqVt7fd6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Preprocessing**\n",
        "\n",
        "This step prepares the dataset for model training by handling outliers, missing values, encoding, scaling, and feature selection.\n",
        "\n",
        "Data Loading\n",
        "* The dataset is loaded from Google Drive for analysis.\n",
        "\n",
        "Outlier Detection\n",
        "* We identify and remove outliers in numerical columns using Z-score filtering.\n",
        "* Data points with a Z-score greater than 3 are removed to reduce noise and improve model performance.\n",
        "\n",
        "Handling Missing Values\n",
        "* Missing values are imputed using the mean for numerical columns, ensuring we retain all rows without introducing gaps in the data.\n",
        "\n",
        "Encoding Categorical Variables\n",
        "* Categorical columns (e.g., `Race`, `Marital Status`) are one-hot encoded, converting them into a numerical format that machine learning models can process.\n",
        "\n",
        "Mapping the Target Variable\n",
        "* The target variable, `Status`, is mapped to binary values (`0` for \"Alive\" and `1` for \"Dead\") for classification.\n",
        "\n",
        "Data Splitting\n",
        "* We split the dataset into training and testing sets with a 70-30 split to train and evaluate the model effectively.\n",
        "\n",
        "Standardization\n",
        "* Features are standardized using `StandardScaler` to ensure uniform scaling, which is crucial for distance-based models like KNN.\n",
        "\n",
        "Feature Selection\n",
        "* Using Information Gain, we calculate feature importance scores to identify the top features that contribute most to predicting survivability.\n"
      ],
      "metadata": {
        "id": "TPF8EIXsz9Qm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DVh8kIvWFW2",
        "outputId": "4832703c-7c7c-4c25-f4bb-d755204b00ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Access the file from Google Drive\n",
        "data = pd.read_csv('/content/drive/My Drive/PA_HW3/Breast_Cancer_dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from scipy import stats\n",
        "\n",
        "# Remove any leading/trailing spaces from column names\n",
        "data.columns = data.columns.str.strip()\n",
        "\n",
        "# Outlier Detection using Z-Score\n",
        "z_scores = np.abs(stats.zscore(data.select_dtypes(include=[np.number])))\n",
        "data_no_outliers = data[(z_scores < 3).all(axis=1)]\n",
        "\n",
        "# Separating target variable\n",
        "y = data[\"Status\"].copy()\n",
        "X = data.drop(columns=[\"Status\"])\n",
        "\n",
        "# Detect categorical columns\n",
        "categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
        "X = pd.get_dummies(X, columns=categorical_columns)\n",
        "\n",
        "if y.dtype == 'object':\n",
        "    y = y.map({\"Alive\": 0, \"Dead\": 1})\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Impute missing values in X_train and X_test\n",
        "imputer = SimpleImputer(strategy=\"mean\")\n",
        "X_train = imputer.fit_transform(X_train)\n",
        "X_test = imputer.transform(X_test)\n",
        "\n",
        "# Standardize the features after imputing missing values\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Feature Selection using Information Gain\n",
        "info_gain = mutual_info_classif(X_train_scaled, y_train)\n",
        "feature_importances = pd.Series(info_gain, index=X.columns).sort_values(ascending=False)\n",
        "top_features = feature_importances[:10].index\n",
        "print(\"Top Features based on Information Gain:\\n\", feature_importances.head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0I6KL2IX7qg",
        "outputId": "2215e82f-ff1f-4167-dc46-968bbdc42c9e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Features based on Information Gain:\n",
            " Survival Months                 0.137639\n",
            "Progesterone Status_Positive    0.027779\n",
            "6th Stage_IIIC                  0.024776\n",
            "Reginol Node Positive           0.024068\n",
            "N Stage_N1                      0.020010\n",
            "N Stage_N3                      0.017142\n",
            "Progesterone Status_Negative    0.014711\n",
            "Tumor Size                      0.014541\n",
            "6th Stage_IIA                   0.013018\n",
            "Grade_3                         0.011348\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dimensionality Reduction with PCA**\n",
        "\n",
        "To simplify the dataset and reduce potential noise, we apply **Principal Component Analysis (PCA)** to transform the features into a lower-dimensional space.\n",
        "\n",
        "* **Purpose**: PCA helps to reduce the number of features while retaining the most important information, which can speed up model training and reduce the risk of overfitting.\n",
        "\n",
        "* **Number of Components**: We choose the top 10 principal components, capturing the majority of the variance in the data.\n",
        "\n",
        "The transformed datasets (`X_train_pca` and `X_test_pca`) are used for model training and testing, providing a more compact representation of the original data.\n"
      ],
      "metadata": {
        "id": "o4zFFNwN4Ee7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using PCA to reduce dimensions\n",
        "pca = PCA(n_components=10)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "Ktov4AOQZZqm"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Modeling**\n",
        "\n",
        "In this step, we train several machine learning algorithms on the preprocessed dataset. Each model has unique strengths and weaknesses, making them suitable for different types of data and tasks. We also implement feature selection techniques, using the most informative features to improve model performance.\n",
        "\n",
        "### 1. K-Nearest Neighbors (KNN)\n",
        "* **Description**: KNN is a distance-based algorithm that classifies data points based on the majority label of their k nearest neighbors.\n",
        "* **Implementation**: We implement KNN from scratch without using any built-in libraries for the classification.\n",
        "* **Pros**: Simple and interpretable; works well for small datasets.\n",
        "* **Cons**: Computationally expensive for large datasets and sensitive to feature scaling.\n",
        "* **Main Hyperparameter**: `k` (number of neighbors).\n",
        "\n",
        "### 2. Naive Bayes\n",
        "* **Description**: A probabilistic classifier based on Bayes' theorem, assuming independence between features.\n",
        "* **Pros**: Fast and efficient; performs well with high-dimensional data.\n",
        "* **Cons**: Assumes feature independence, which may not hold in all datasets, potentially reducing accuracy.\n",
        "* **Main Hyperparameter**: `var_smoothing` (for Gaussian Naive Bayes).\n",
        "\n",
        "### 3. Decision Tree (C4.5)\n",
        "* **Description**: A tree-based model that splits data into branches based on feature values, making decisions at each node.\n",
        "* **Pros**: Interpretable and flexible, works with both numerical and categorical data.\n",
        "* **Cons**: Prone to overfitting, especially with deep trees.\n",
        "* **Main Hyperparameter**: `max_depth` (controls the depth of the tree), `criterion` (split criteria, e.g., Gini or Entropy).\n",
        "\n",
        "### 4. Random Forest\n",
        "* **Description**: An ensemble of decision trees that combines predictions from multiple trees to improve accuracy and reduce overfitting.\n",
        "* **Pros**: Robust to overfitting; can handle high-dimensional data and provides feature importance insights.\n",
        "* **Cons**: Less interpretable than a single decision tree; computationally intensive with many trees.\n",
        "* **Main Hyperparameters**: `n_estimators` (number of trees), `max_depth` (depth of each tree).\n",
        "\n",
        "### 5. Gradient Boosting\n",
        "* **Description**: An ensemble method that builds trees sequentially, with each tree correcting the errors of the previous ones.\n",
        "* **Pros**: High accuracy and suitable for imbalanced data.\n",
        "* **Cons**: Computationally intensive; prone to overfitting if not carefully tuned.\n",
        "* **Main Hyperparameters**: `n_estimators` (number of boosting stages), `learning_rate` (controls the contribution of each tree).\n",
        "\n",
        "### 6. Neural Network (Multi-layer Perceptron)\n",
        "* **Description**: A multi-layer perceptron that learns complex patterns through layers of interconnected neurons.\n",
        "* **Pros**: Can capture complex, non-linear relationships in the data.\n",
        "* **Cons**: Computationally expensive; requires more data and is prone to overfitting if not regularized.\n",
        "* **Main Hyperparameters**: `hidden_layer_sizes` (number of neurons in each layer), `activation` (activation function), `max_iter` (maximum number of training iterations).\n"
      ],
      "metadata": {
        "id": "2J9IbccD18Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleKNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X_train = X\n",
        "        self.y_train = y\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for row in X:\n",
        "            distances = np.linalg.norm(self.X_train - row, axis=1)\n",
        "            k_indices = np.argsort(distances)[:self.k]\n",
        "            k_nearest_labels = self.y_train[k_indices]\n",
        "            predictions.append(np.bincount(k_nearest_labels).argmax())\n",
        "        return predictions\n",
        "\n",
        "knn = SimpleKNN(k=3)\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "predictions = {}\n",
        "predictions[\"KNN\"] = knn.predict(X_test_scaled)\n"
      ],
      "metadata": {
        "id": "E2FYRsNHZfQp"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "nb = GaussianNB()\n",
        "dt = DecisionTreeClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "gb = GradientBoostingClassifier()\n",
        "nn = MLPClassifier()\n",
        "\n",
        "models = {\n",
        "    \"Naive Bayes\": nb,\n",
        "    \"Decision Tree\": dt,\n",
        "    \"Random Forest\": rf,\n",
        "    \"Gradient Boosting\": gb,\n",
        "    \"Neural Network\": nn\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    predictions[name] = model.predict(X_test_scaled)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-txT7twDcGcU",
        "outputId": "9d5a50dd-606a-4222-a3b0-ebc4ecf5437b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Hyperparameter Tuning**\n",
        "\n",
        "In this step, we perform hyperparameter tuning on two of the models: **Random Forest** and **Gradient Boosting**. By using **Grid Search** with cross-validation, we explore different combinations of hyperparameters to identify the optimal settings for each model. This process helps to improve model performance by finding the best configuration of parameters.\n",
        "\n",
        "### 1. Random Forest Hyperparameter Tuning\n",
        "* **Parameters Tuned**:\n",
        "  - `n_estimators`: Number of trees in the forest, tested values are 50 and 100.\n",
        "  - `max_depth`: Maximum depth of each tree, tested values are `None` (unrestricted depth) and 10.\n",
        "\n",
        "### 2. Gradient Boosting Hyperparameter Tuning\n",
        "* **Parameters Tuned**:\n",
        "  - `n_estimators`: Number of boosting stages, tested values are 50 and 100.\n",
        "  - `learning_rate`: Step size for each stage, tested values are 0.1 and 0.01.\n"
      ],
      "metadata": {
        "id": "snpJ3OL84bYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "tuned_params = {\n",
        "    \"Random Forest\": {'n_estimators': [50, 100], 'max_depth': [None, 10]},\n",
        "    \"Gradient Boosting\": {'n_estimators': [50, 100], 'learning_rate': [0.1, 0.01]}\n",
        "}\n",
        "\n",
        "best_params = {}\n",
        "for name, params in tuned_params.items():\n",
        "    search = GridSearchCV(models[name], params, cv=3)\n",
        "    search.fit(X_train_scaled, y_train)\n",
        "    best_params[name] = search.best_params_\n",
        "\n",
        "for model, params in best_params.items():\n",
        "    print(f\"Best parameters for {model}: {params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeN1Jo8ccShR",
        "outputId": "8a72bce8-7d2c-41c9-af28-094d7e92e035"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters for Random Forest: {'max_depth': 10, 'n_estimators': 100}\n",
            "Best parameters for Gradient Boosting: {'learning_rate': 0.1, 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Results**\n",
        "\n",
        "In this step, we evaluate the performance of each model using several metrics: **Accuracy**, **Precision**, **Recall**, and **F1 Score**. These metrics provide a comprehensive view of each model's effectiveness in predicting breast cancer survivability. Additionally, for models that allow feature importance evaluation, we identify the most important features contributing to the classification.\n",
        "\n",
        "### Model Performance\n",
        "\n",
        "The table below summarizes the performance of each model:\n",
        "\n",
        "| Model             | Accuracy | Precision | Recall | F1 Score |\n",
        "|-------------------|----------|-----------|--------|----------|\n",
        "| KNN               | 0.836093 | 0.396396  | 0.251429 | 0.307692 |\n",
        "| Naive Bayes       | 0.816225 | 0.385366  | 0.451429 | 0.415789 |\n",
        "| Decision Tree     | 0.852649 | 0.492386  | 0.554286 | 0.521505 |\n",
        "| Random Forest     | 0.906457 | 0.810000  | 0.462857 | 0.589091 |\n",
        "| Gradient Boosting | 0.912252 | 0.805310  | 0.520000 | 0.631944 |\n",
        "| Neural Network    | 0.888245 | 0.651515  | 0.491429 | 0.560261 |\n",
        "\n",
        "### Top Features by Importance (Random Forest)\n",
        "\n",
        "Using the Random Forest model, we identify the top 10 features based on their importance scores:\n",
        "\n",
        "| Feature                      | Importance |\n",
        "|------------------------------|------------|\n",
        "| Survival Months              | 0.337096   |\n",
        "| Age                          | 0.111125   |\n",
        "| Regional Node Examined       | 0.094744   |\n",
        "| Tumor Size                   | 0.091198   |\n",
        "| Reginol Node Positive        | 0.073965   |\n",
        "| Marital Status_Married       | 0.021563   |\n",
        "| Race_White                   | 0.016464   |\n",
        "| 6th Stage_IIIC               | 0.014079   |\n",
        "| Progesterone Status_Negative | 0.013580   |\n",
        "| Estrogen Status_Negative     | 0.013064   |\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The objective of this project was to predict breast cancer survivability. Based on the results, **Gradient Boosting** and **Random Forest** achieved the highest accuracy and F1 scores, indicating strong predictive power for this task. These models are well-suited for capturing complex patterns in the data, making them effective for this classification problem.\n",
        "\n",
        "The most informative features (e.g., **Survival Months**, **Age**, **Regional Node Examined**) align with clinical expectations, suggesting that the model insights are consistent with real-world factors that impact survivability.\n",
        "\n",
        "In summary, we were able to answer the initial question of predicting breast cancer survivability effectively, especially with the use of ensemble models.\n"
      ],
      "metadata": {
        "id": "3vl-8Zmk7It3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "for name, preds in predictions.items():\n",
        "    results[name] = {\n",
        "        \"Accuracy\": accuracy_score(y_test, preds),\n",
        "        \"Precision\": precision_score(y_test, preds),\n",
        "        \"Recall\": recall_score(y_test, preds),\n",
        "        \"F1 Score\": f1_score(y_test, preds)\n",
        "    }\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "\n",
        "# Display Feature Importance for Random Forest\n",
        "if \"Random Forest\" in models:\n",
        "    feature_importance = pd.Series(models[\"Random Forest\"].feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "    print(\"Top Features by Importance in Random Forest:\\n\", feature_importance.head(10))\n",
        "\n",
        "print(\"Model Performance:\\n\", results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQlB7TUznQPK",
        "outputId": "0cd04e0d-86b0-412b-877c-606ca7e31bda"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top Features by Importance in Random Forest:\n",
            " Survival Months                 0.337096\n",
            "Age                             0.111125\n",
            "Regional Node Examined          0.094744\n",
            "Tumor Size                      0.091198\n",
            "Reginol Node Positive           0.073965\n",
            "Marital Status_Married          0.021563\n",
            "Race_White                      0.016464\n",
            "6th Stage_IIIC                  0.014079\n",
            "Progesterone Status_Negative    0.013580\n",
            "Estrogen Status_Negative        0.013064\n",
            "dtype: float64\n",
            "Model Performance:\n",
            "                    Accuracy  Precision    Recall  F1 Score\n",
            "KNN                0.836093   0.396396  0.251429  0.307692\n",
            "Naive Bayes        0.816225   0.385366  0.451429  0.415789\n",
            "Decision Tree      0.852649   0.492386  0.554286  0.521505\n",
            "Random Forest      0.906457   0.810000  0.462857  0.589091\n",
            "Gradient Boosting  0.912252   0.805310  0.520000  0.631944\n",
            "Neural Network     0.888245   0.651515  0.491429  0.560261\n"
          ]
        }
      ]
    }
  ]
}